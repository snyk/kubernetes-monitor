commands:
    install_helm:
        description: Install Helm
        steps:
            - run:
                command: |
                    curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
                    chmod 700 get_helm.sh
                    ./get_helm.sh
                name: Install Helm
    install_python_requests:
        description: Install requests library
        steps:
            - run:
                command: |
                    sudo apt update
                    sudo apt install python3-requests
                when: always
    setup_node16:
        description: Setup Node 16
        steps:
            - run:
                command: |
                    export NVM_DIR="/opt/circleci/.nvm"
                    [ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"
                    nvm install v16
                    npm ci
                    echo 'export NVM_DIR="/opt/circleci/.nvm"' >> $BASH_ENV
                    echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> $BASH_ENV
                    echo 'nvm alias default v16' >> $BASH_ENV
jobs:
    aks_integration_tests:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        resource_class: large
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - azure-cli/install
            - run:
                command: mkdir -p /tmp/logs/test/integration/aks
                name: Create temp dir for logs
            - run:
                command: |
                    npm ci &&
                    export KUBERNETES_MONITOR_IMAGE_NAME_AND_TAG=$(./scripts/circleci-jobs/setup-integration-tests.py)
                    .circleci/do-exclusively --branch staging --job ${CIRCLE_JOB} npm run test:integration:aks:yaml
                name: Integration tests AKS
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
            - store_artifacts:
                path: /tmp/logs/test/integration/aks
        working_directory: ~/kubernetes-monitor
    build_and_upload_operator:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/python:3.10
        steps:
            - checkout
            - setup_remote_docker
            - install_python_requests
            - run:
                command: |
                    scripts/operator/download_operator_sdk.py
                    scripts/operator/download_operator_package_manager.py
                name: Download Operator SDK and Operator Package Manager
            - run:
                command: |
                    export IMAGE_TAG=$([[ "$CIRCLE_BRANCH" == "staging" ]] && echo "staging-candidate" || echo "discardable")
                    OPERATOR_TAG="${IMAGE_TAG}-ubi8-${CIRCLE_SHA1:0:8}"
                    MONITOR_TAG="${IMAGE_TAG}-ubi8-${CIRCLE_SHA1:0:8}"
                    scripts/operator/create_operator_and_push.py "${OPERATOR_TAG}" "${MONITOR_TAG}" "${DOCKERHUB_USER}" "${DOCKERHUB_PASSWORD}"
                    echo "export OPERATOR_TAG=$OPERATOR_TAG" >> $BASH_ENV
                name: Create Operator and push Operator image to DockerHub
            - snyk/scan:
                docker-image-name: snyk/kubernetes-operator:${OPERATOR_TAG}
                monitor-on-build: false
                severity-threshold: critical
                target-file: snyk-operator/build/Dockerfile
            - run:
                command: |
                    export IMAGE_TAG=$([[ "$CIRCLE_BRANCH" == "staging" ]] && echo "staging-candidate" || echo "discardable")
                    export SNYK_MONITOR_IMAGE_TAG="${IMAGE_TAG}-ubi8-${CIRCLE_SHA1:0:8}"
                    export SNYK_OPERATOR_VERSION="0.0.1-ubi8-${CIRCLE_SHA1:0:8}"
                    export SNYK_OPERATOR_IMAGE_TAG="${SNYK_MONITOR_IMAGE_TAG}"
                    OPERATOR_PATH=$(scripts/operator/package_operator_bundle.py "${SNYK_OPERATOR_VERSION}" "${SNYK_OPERATOR_IMAGE_TAG}" "${SNYK_MONITOR_IMAGE_TAG}")
                    echo "export OPERATOR_PATH=$OPERATOR_PATH" >> $BASH_ENV
                name: Package Operator Bundle
            - run:
                command: |
                    export OPERATOR_DIR=$OPERATOR_PATH
                    export PACKAGE_VERSION="0.0.1-ubi8-${CIRCLE_SHA1:0:8}"
                    scripts/operator/create_operator_bundle_and_index_and_push.py "${OPERATOR_DIR}" "${PACKAGE_VERSION}" "${DOCKERHUB_USER}" "${DOCKERHUB_PASSWORD}"
                name: Create Operator Bundle and Index and push to Docker Hub
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    build_image:
        machine:
            image: ubuntu-2004:202111-02
        steps:
            - checkout
            - install_python_requests
            - run:
                command: |
                    IMAGE_TAG=$([[ "$CIRCLE_BRANCH" == "staging" ]] && echo "staging-candidate" || echo "discardable")
                    IMAGE_NAME_CANDIDATE=snyk/kubernetes-monitor:${IMAGE_TAG}-${CIRCLE_SHA1}
                    IMAGE_NAME_CANDIDATE_UBI8=snyk/kubernetes-monitor:${IMAGE_TAG}-ubi8-${CIRCLE_SHA1:0:8}
                    echo "export IMAGE_NAME_CANDIDATE=$IMAGE_NAME_CANDIDATE" >> $BASH_ENV
                    echo "export IMAGE_NAME_CANDIDATE_UBI8=$IMAGE_NAME_CANDIDATE_UBI8" >> $BASH_ENV
                name: Export environment variables
            - run:
                command: |
                    docker login --username ${DOCKERHUB_USER} --password ${DOCKERHUB_PASSWORD}
                    ./scripts/docker/build-image.sh ${IMAGE_NAME_CANDIDATE}
                    ./scripts/docker/build-image-ubi8.sh ${IMAGE_NAME_CANDIDATE_UBI8}
                name: Build image
            - snyk/scan:
                additional-arguments: --project-name=alpine
                docker-image-name: ${IMAGE_NAME_CANDIDATE}
                monitor-on-build: false
                severity-threshold: high
                target-file: Dockerfile
            - snyk/scan:
                additional-arguments: --project-name=ubi8
                docker-image-name: ${IMAGE_NAME_CANDIDATE_UBI8}
                monitor-on-build: false
                severity-threshold: critical
                target-file: Dockerfile.ubi8
            - run:
                command: |
                    docker push ${IMAGE_NAME_CANDIDATE}
                    docker push ${IMAGE_NAME_CANDIDATE_UBI8}
                name: Push image
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    build_image_snyk_main:
        machine:
            image: ubuntu-2004:202111-02
        steps:
            - checkout
            - install_python_requests
            - run:
                command: |
                    IMAGE_TAG=$([[ "$CIRCLE_BRANCH" == "staging" ]] && echo "staging-candidate" || echo "discardable")
                    IMAGE_NAME_CANDIDATE=gcr.io/snyk-main/kubernetes-monitor:${IMAGE_TAG}-${CIRCLE_SHA1}
                    IMAGE_NAME_CANDIDATE_UBI8=gcr.io/snyk-main/kubernetes-monitor:${IMAGE_TAG}-ubi8-${CIRCLE_SHA1:0:8}
                    echo "export IMAGE_NAME_CANDIDATE=$IMAGE_NAME_CANDIDATE" >> $BASH_ENV
                    echo "export IMAGE_NAME_CANDIDATE_UBI8=$IMAGE_NAME_CANDIDATE_UBI8" >> $BASH_ENV
                name: Export environment variables
            - run:
                command: |
                    echo $GCLOUD_GCR_BUILDER | docker login -u _json_key --password-stdin https://gcr.io/snyk-main
                    ./scripts/docker/build-image.sh ${IMAGE_NAME_CANDIDATE}
                    ./scripts/docker/build-image-ubi8.sh ${IMAGE_NAME_CANDIDATE_UBI8}
                name: Build image
            - snyk/scan:
                additional-arguments: --project-name=alpine
                docker-image-name: ${IMAGE_NAME_CANDIDATE}
                monitor-on-build: false
                severity-threshold: high
                target-file: Dockerfile
            - snyk/scan:
                additional-arguments: --project-name=ubi8
                docker-image-name: ${IMAGE_NAME_CANDIDATE_UBI8}
                monitor-on-build: false
                severity-threshold: critical
                target-file: Dockerfile.ubi8
            - run:
                command: |
                    docker push ${IMAGE_NAME_CANDIDATE}
                    docker push ${IMAGE_NAME_CANDIDATE_UBI8}
                name: Push image
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    code_formatter:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: |
                    npm run format:check
                name: code formatter check
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    deploy_to_dev:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/node:16.13
        steps:
            - checkout
            - install_python_requests
            - run:
                command: ./scripts/circleci-jobs/deploy_to_dev.sh
                name: Deploy to dev
            - run:
                command: ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    deploy_to_prod:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/node:16.13
        steps:
            - checkout
            - install_python_requests
            - run:
                command: ./scripts/circleci-jobs/deploy_to_prod.sh
                name: Deploy to prod
            - run:
                command: ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    deploy_to_sysdig_integration_cluster:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/base:stable
        steps:
            - checkout
            - run:
                command: |
                    LATEST_KUBECTL_VERSION=$(curl -L -s https://dl.k8s.io/release/stable.txt)
                    curl -LO "https://dl.k8s.io/release/${LATEST_KUBECTL_VERSION}/bin/linux/amd64/kubectl"
                    curl -LO "https://dl.k8s.io/${LATEST_KUBECTL_VERSION}/bin/linux/amd64/kubectl.sha256"
                    echo "$(<kubectl.sha256) kubectl" | sha256sum --check
                    sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
                    # Ensure the kubectl command is runnable
                    kubectl version --client
                    # Prepare kubeconfig to point to the cluster
                    mkdir ~/.kube || true
                    printf "%s" "${SYSDIG_KUBECONFIG}" | base64 -d > ~/.kube/config
                name: Install and prepare kubectl
            - run:
                command: |
                    curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
                    chmod 700 get_helm.sh
                    ./get_helm.sh
                    # Ensure the Helm command is runnable
                    helm version
                name: Install Helm
            - run:
                command: |
                    LATEST_TAG_WITH_V=`git describe --abbrev=0 --tags ${CIRCLE_SHA1}`
                    LATEST_TAG=${LATEST_TAG_WITH_V:1}-approved
                    ./scripts/slack/notify_deploy.py $LATEST_TAG sysdig-integration-cluster
                    helm upgrade --install snyk-monitor ./snyk-monitor --namespace snyk-monitor --set image.tag=${LATEST_TAG} --set clusterName="Sysdig cluster" --set sysdig.enabled=true
                name: Deploy to shared Sysdig cluster
            - run:
                command: ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    eks_integration_tests:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        resource_class: large
        steps:
            - checkout
            - install_python_requests
            - setup_node16
            - aws-cli/install:
                override-installed: true
            - run:
                command: mkdir -p /tmp/logs/test/integration/eks
                name: Create temp dir for logs
            - run:
                command: |
                    npm ci &&
                    export KUBERNETES_MONITOR_IMAGE_NAME_AND_TAG=$(./scripts/circleci-jobs/setup-integration-tests.py)
                    .circleci/do-exclusively --branch staging --job ${CIRCLE_JOB} npm run test:integration:eks:yaml
                name: Integration tests EKS
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
            - store_artifacts:
                path: /tmp/logs/test/integration/eks
        working_directory: ~/kubernetes-monitor
    integration_tests:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        resource_class: large
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: mkdir -p /tmp/logs/test/integration/kind
                name: create temp dir for logs
            - run:
                command: |
                    export KUBERNETES_MONITOR_IMAGE_NAME_AND_TAG=$(./scripts/circleci-jobs/setup-integration-tests.py)
                    npm run test:integration:kind:yaml
                name: Integration tests
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
            - store_artifacts:
                path: /tmp/logs/test/integration/kind
        working_directory: ~/kubernetes-monitor
    integration_tests_helm:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        resource_class: large
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: mkdir -p /tmp/logs/test/integration/kind-helm
                name: Create temporary directory for logs
            - run:
                command: |
                    export KUBERNETES_MONITOR_IMAGE_NAME_AND_TAG=$(./scripts/circleci-jobs/setup-integration-tests.py)
                    npm run test:integration:kind:helm
                name: Integration tests with Helm deployment
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
            - store_artifacts:
                path: /tmp/logs/test/integration/kind-helm
        working_directory: ~/kubernetes-monitor
    integration_tests_operator_on_k8s:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
            resource_class: large
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: mkdir -p /tmp/logs/test/integration/kind-olm-operator
                name: Create temporary directory for logs
            - run:
                command: |
                    export OPERATOR_VERSION="0.0.1-ubi8-${CIRCLE_SHA1:0:8}"
                    export IMAGE_TAG_UBI_SUFFIX="-ubi8"
                    export KUBERNETES_MONITOR_IMAGE_NAME_AND_TAG=$(./scripts/circleci-jobs/setup-integration-tests.py)
                    .circleci/do-exclusively --branch staging --job ${CIRCLE_JOB} npm run test:integration:kindolm:operator
                name: Operator integration tests on vanilla Kubernetes
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
            - store_artifacts:
                path: /tmp/logs/test/integration/kind-olm-operator
        working_directory: ~/kubernetes-monitor
    integration_tests_proxy:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        resource_class: large
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: mkdir -p /tmp/logs/test/integration/proxy
                name: Create temporary directory for logs
            - run:
                command: |
                    export KUBERNETES_MONITOR_IMAGE_NAME_AND_TAG=$(./scripts/circleci-jobs/setup-integration-tests.py)
                    npm run test:integration:kind:proxy
                name: Integration tests with proxy
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
            - store_artifacts:
                path: /tmp/logs/test/integration/proxy
        working_directory: ~/kubernetes-monitor
    lint:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: |
                    npm run lint
                name: lint
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    openshift4_integration_tests:
        machine:
            docker_layer_caching: true
            image: ubuntu-2204:current
            resource_class: large
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: mkdir -p /tmp/logs/test/integration/openshift4
                name: create temp dir for logs
            - run:
                command: |
                    export OPERATOR_VERSION="0.0.1-ubi8-${CIRCLE_SHA1:0:8}"
                    export IMAGE_TAG_UBI_SUFFIX="-ubi8"
                    export KUBERNETES_MONITOR_IMAGE_NAME_AND_TAG=$(./scripts/circleci-jobs/setup-integration-tests.py)
                    .circleci/do-exclusively --branch staging --job ${CIRCLE_JOB} npm run test:integration:openshift4:operator
                name: Integration tests OpenShift 4
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
            - store_artifacts:
                path: /tmp/logs/test/integration/openshift4
        working_directory: ~/kubernetes-monitor
    operator_upgrade_tests:
        description: |
            Deploys a previously released version of the snyk-operator.
            Subsequently upgrades the Operator with a new version that is intended
            to be released. If the Operator reaches the running state in both cases,
            we can assume that it's able to upgrade.
        executor: redhat-openshift/default
        steps:
            - checkout
            - run:
                command: |
                    sudo apt update
                    sudo apt install -y uuid-runtime make
                    python -m pip install requests pyyaml
                    python scripts/operator/download_operator_sdk.py
                    # The machine executor uses an old debian version, the latest OPM 1.16.1 requires a later version of GLIBC that the current machine cannot have.
                    # So instead of using the (currently) latest version, we have to use an older one that references an older GLIBC.
                    OPM_VERSION=v1.13.0 python scripts/operator/download_operator_package_manager.py
                    curl -L https://github.com/openshift/okd/releases/download/4.7.0-0.okd-2021-03-28-152009/openshift-client-linux-4.7.0-0.okd-2021-03-28-152009.tar.gz | tar xfz - -- oc
                    sudo mv oc /usr/local/bin/oc
                    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
                    sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
                name: Install required packages
            - install_python_requests
            - setup_remote_docker
            - run: docker login --username ${DOCKERHUB_USER} --password ${DOCKERHUB_PASSWORD}
            - run:
                command: |
                    oc login --token="${OPENSHIFT4_PASSWORD}" --server="${OPENSHIFT4_CLUSTER_URL}" --kubeconfig="$(pwd)/kubeconfig"
                    echo "export KUBECONFIG=$(pwd)/kubeconfig" >> $BASH_ENV
                name: Login and update KUBECONFIG
            - run:
                command: |
                    source $BASH_ENV
                    set -xeo pipefail

                    OPERATOR_REPO_URL="https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-prod/main/operators/snyk-operator/snyk-operator.package.yaml"

                    OPERATOR_VERSION=$(python ./scripts/operator/get_last_published_operator_version.py $OPERATOR_REPO_URL)

                    echo "Currently released embedded version is: ${OPERATOR_VERSION}"
                    echo "export OPERATOR_VERSION=${OPERATOR_VERSION}" >> $BASH_ENV
                name: Get last released Operator version
            - run:
                command: |
                    source $BASH_ENV
                    LATEST_TAG_WITH_V=`git describe --abbrev=0 --tags ${CIRCLE_SHA1}`
                    LATEST_TAG=${LATEST_TAG_WITH_V:1}
                    echo "export LATEST_TAG=${LATEST_TAG}" >> $BASH_ENV
                description: |
                    This tag is used to identify the Operator version we are going to upgrade to.
                name: Get latest snyk-monitor tag
            - run:
                command: |
                    source $BASH_ENV
                    if [[ "${LATEST_TAG}" == "${OPERATOR_VERSION}" ]]; then
                      echo "export NOTHING_TO_TEST=true" >> $BASH_ENV
                      exit 1
                    fi
                name: End tests early if no new Operator is to be released
            - run:
                command: |
                    source $BASH_ENV
                    set -xeo pipefail

                    # Package Operator Bundle to be uploaded to Docker Hub
                    SNYK_OPERATOR_IMAGE_TAG=${OPERATOR_VERSION}
                    SNYK_MONITOR_IMAGE_TAG=${OPERATOR_VERSION}-ubi8
                    OPERATOR_DIR=$(python scripts/operator/package_operator_bundle.py $OPERATOR_VERSION $SNYK_OPERATOR_IMAGE_TAG $SNYK_MONITOR_IMAGE_TAG)

                    python scripts/operator/create_operator_bundle_and_index_and_push.py $OPERATOR_DIR $OPERATOR_VERSION $DOCKERHUB_USER $DOCKERHUB_PASSWORD
                description: "Even though the Operator is released to the\nredhat-openshift-ecosystem/community-operators-prod repo, we can \nreproduce it locally using our packaged scripts. This also helps us\ntest the upgrade by pushing all tested Operators to our Quay repo.\n"
                name: Package Operator Bundle and push to Docker Hub
            - run:
                command: |
                    set +e
                    kubectl patch customresourcedefinition snykmonitors.charts.helm.k8s.io -p '{"metadata":{"finalizers":[]}}' --type=merge -n snyk-monitor
                    kubectl patch snykmonitors.charts.helm.k8s.io snyk-monitor -p '{"metadata":{"finalizers":[]}}' --type=merge -n snyk-monitor
                    kubectl delete customresourcedefinition snykmonitors.charts.helm.k8s.io
                    kubectl delete catalogsource snyk-operator -n openshift-marketplace
                    kubectl delete clusterrolebinding snyk-monitor
                    kubectl delete clusterrole snyk-monitor
                    kubectl delete --all all,sa,cm,secret,pvc -n services
                    kubectl delete --all all,sa,cm,secret,pvc -n snyk-monitor
                    kubectl delete namespace services
                    kubectl delete namespace snyk-monitor
                    # Force a success status code, otherwise Bash will implicitly return the last command's code, which can be 1.
                    true
                name: Remove existing cluster resources if present
            - run:
                command: |
                    set -xo pipefail
                    set +e

                    ns=$(kubectl get ns snyk-monitor --no-headers --output=go-template={{.metadata.name}} 2>/dev/null)

                    if [[ -z "${ns}" ]]; then
                      echo "snyk-monitor namespace not found, creating..."
                      kubectl create ns snyk-monitor
                    fi

                    set -e
                    INTEGRATION_ID=$(uuidgen)
                    kubectl create secret generic snyk-monitor -n snyk-monitor --from-literal=integrationId=${INTEGRATION_ID} --from-literal=dockercfg.json={}
                name: Configure snyk-monitor namespace
            - run:
                command: |
                    source $BASH_ENV
                    set -xe

                    sed -i.bak "s|TAG_OVERRIDE|${OPERATOR_VERSION}|g" ./test/fixtures/operator/catalog-source.yaml
                    kubectl apply -f ./test/fixtures/operator/catalog-source.yaml

                    kubectl apply -f ./test/fixtures/operator/installation.yaml
                    sleep 120
                    kubectl get pods -n snyk-monitor --no-headers | \
                      grep "snyk-operator" | \
                      awk 'END { if (NR==0) exit 1; else print $1 }' | \
                      xargs -I{} kubectl wait pod/{} -n snyk-monitor --timeout 60s --for condition=Ready
                name: Install Operator
            - run:
                command: |
                    set -o pipefail

                    kubectl apply -f ./test/fixtures/operator/custom-resource.yaml
                    sleep 120

                    kubectl get pods -n snyk-monitor --no-headers | \
                      grep "snyk-monitor" | \
                      awk 'END { if (NR==0) exit 1; else print $1 }' | \
                      xargs -I{} kubectl wait pod/{} -n snyk-monitor --timeout 60s --for condition=Ready
                name: Deploy snyk-monitor resource
            - run:
                command: |
                    source $BASH_ENV
                    set -eo pipefail

                    REPLACES_VERSION=${OPERATOR_VERSION}
                    SNYK_MONITOR_TAG=${LATEST_TAG}-ubi8
                    OPERATOR_DIR=$(python scripts/operator/package_operator_bundle.py "${LATEST_TAG}" "${LATEST_TAG}" "${SNYK_MONITOR_TAG}" "${REPLACES_VERSION}")
                    python scripts/operator/create_operator_bundle_and_index_and_push.py $OPERATOR_DIR $LATEST_TAG $DOCKERHUB_USER $DOCKERHUB_PASSWORD $REPLACES_VERSION
                description: Now we are testing that any upgrades to the Operator are detected and applied in the cluster.
                name: Package Operator Bundle upgrade and push to Docker Hub
            - run:
                command: |
                    source $BASH_ENV
                    set -xeo pipefail

                    # Replace the catalog source with the latest bundled version of snyk-monitor, this is what initiates the upgrade
                    sed -i.bak "s|${OPERATOR_VERSION}|${LATEST_TAG}|g" ./test/fixtures/operator/catalog-source.yaml
                    kubectl apply -f ./test/fixtures/operator/catalog-source.yaml

                    ATTEMPTS=120
                    SLEEP_SECONDS_BETWEEN_ATTEMPTS=5
                    # total = 10 minutes wait time

                    # Periodically poll if the snyk-monitor has upgraded
                    for (( attempt=1; attempt<ATTEMPTS; attempt++))
                    do
                      # Grab the tag of the snyk-monitor container image

                      VERSION=$(kubectl get pods -n snyk-monitor --no-headers | \
                        grep "snyk-monitor" | \
                        awk 'END { if (NR==0) print ""; else print $1 }' | \
                        xargs '-I{}' kubectl get pod '{}' -n snyk-monitor -o 'jsonpath={..containers[*].image}' | \
                        awk '{print $1}' | \
                        grep -oE "[0-9]{1}\.[0-9]{1,2}\.[0-9]{1,3}-ubi8$" \
                        || echo "0.0.0")

                      # Break out of the polling if the tag matches the one we want to upgrade to.
                      if [[ "${VERSION}" == "${LATEST_TAG}-ubi8" ]]; then
                        break
                      fi

                      # Otherwise keep polling
                      sleep $SLEEP_SECONDS_BETWEEN_ATTEMPTS
                    done

                    SNYK_MONITOR_POD=$(kubectl get pods -n snyk-monitor --no-headers | \
                      grep "snyk-monitor" | \
                      awk 'END { if (NR==0) exit 101; else print $1 }')

                    # If we polled for 5 minutes and the snyk-monitor still hasn't upgraded, fail the current job.
                    if [[ "${VERSION}" != "${LATEST_TAG}-ubi8" ]]; then
                      &>2 echo "versions (${VERSION}) does not match expected (${LATEST_TAG})!"

                      kubectl describe pod ${SNYK_MONITOR_POD} -n snyk-monitor
                      kubectl describe catalogsource snyk-operator -n openshift-marketplace
                      kubectl get snykmonitors.charts.helm.k8s.io -n snyk-monitor -o yaml

                      exit 102
                    fi

                    # We need to wait for the Pod to become Ready
                    kubectl wait pod/${SNYK_MONITOR_POD} -n snyk-monitor --timeout 120s --for condition=Ready

                    echo "Update complete!"
                name: Upgrade Operator and check that snyk-monitor also upgraded
            - run:
                command: |
                    source $BASH_ENV
                    ./scripts/slack/notify_success_openshift_upgrade.py "${OPERATOR_VERSION}" "${LATEST_TAG}"
                name: Notify Slack on successful upgrade
            - run:
                command: |
                    set +e

                    kubectl delete -f ./test/fixtures/operator/catalog-source.yaml
                    kubectl delete -f ./test/fixtures/operator/installation.yaml

                    kubectl patch customresourcedefinition snykmonitors.charts.helm.k8s.io -p '{"metadata":{"finalizers":[]}}' --type=merge -n snyk-monitor
                    kubectl patch snykmonitors.charts.helm.k8s.io snyk-monitor -p '{"metadata":{"finalizers":[]}}' --type=merge -n snyk-monitor
                    kubectl delete -f ./test/fixtures/operator/custom-resource.yaml
                    kubectl delete clusterrolebinding snyk-monitor
                    kubectl delete clusterrole snyk-monitor
                    kubectl delete --all all,sa,cm,secret,pvc -n services
                    kubectl delete --all all,sa,cm,secret,pvc -n snyk-monitor
                    kubectl delete namespace snyk-monitor
                    kubectl delete namespace services
                    # Force a success status code, otherwise Bash will implicitly return the last command's code, which can be 1.
                    true
                name: Cleanup
                when: always
            - run:
                command: |
                    if [[ "${NOTHING_TO_TEST}" != "true" ]]; then
                      ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                    fi
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    publish:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/node:16.13
        steps:
            - checkout
            - setup_remote_docker
            - install_python_requests
            - install_helm
            - run:
                command: |
                    LATEST_TAG_WITH_V=`git describe --abbrev=0 --tags ${CIRCLE_SHA1}`
                    LATEST_TAG=${LATEST_TAG_WITH_V:1}
                    IMAGE_NAME_APPROVED=snyk/kubernetes-monitor:${LATEST_TAG}-approved
                    IMAGE_NAME_PUBLISHED=snyk/kubernetes-monitor:${LATEST_TAG}
                    IMAGE_NAME_APPROVED_UBI8=snyk/kubernetes-monitor:${LATEST_TAG}-ubi8-approved
                    IMAGE_NAME_PUBLISHED_UBI8=snyk/kubernetes-monitor:${LATEST_TAG}-ubi8
                    echo "export LATEST_TAG=${LATEST_TAG}" >> $BASH_ENV
                    echo "export IMAGE_NAME_APPROVED=${IMAGE_NAME_APPROVED}" >> $BASH_ENV
                    echo "export IMAGE_NAME_PUBLISHED=${IMAGE_NAME_PUBLISHED}" >> $BASH_ENV
                    echo "export IMAGE_NAME_APPROVED_UBI8=${IMAGE_NAME_APPROVED_UBI8}" >> $BASH_ENV
                    echo "export IMAGE_NAME_PUBLISHED_UBI8=${IMAGE_NAME_PUBLISHED_UBI8}" >> $BASH_ENV
                name: Export environment variables
            - snyk/scan:
                monitor-on-build: true
                severity-threshold: high
            - snyk/scan:
                additional-arguments: --project-name=alpine
                docker-image-name: ${IMAGE_NAME_APPROVED}
                monitor-on-build: true
                severity-threshold: high
                target-file: Dockerfile
            - snyk/scan:
                additional-arguments: --project-name=ubi8
                docker-image-name: ${IMAGE_NAME_APPROVED_UBI8}
                monitor-on-build: true
                severity-threshold: critical
                target-file: Dockerfile.ubi8
            - run:
                command: |
                    docker login --username ${DOCKERHUB_USER} --password ${DOCKERHUB_PASSWORD} &&
                    docker pull ${IMAGE_NAME_APPROVED} &&
                    docker tag ${IMAGE_NAME_APPROVED} ${IMAGE_NAME_PUBLISHED} &&
                    docker push ${IMAGE_NAME_PUBLISHED} &&
                    docker pull ${IMAGE_NAME_APPROVED_UBI8} &&
                    docker tag ${IMAGE_NAME_APPROVED_UBI8} ${IMAGE_NAME_PUBLISHED_UBI8} &&
                    docker push ${IMAGE_NAME_PUBLISHED_UBI8} &&
                    ./scripts/slack/notify_push.py ${IMAGE_NAME_PUBLISHED} &&
                    ./scripts/slack/notify_push.py ${IMAGE_NAME_PUBLISHED_UBI8} &&
                    ./scripts/publish-gh-pages.sh ${LATEST_TAG}
                name: Publish
            - run:
                command: |
                    RELEASE_VERSION=v0.15.1
                    DOWNLOAD_LOCATION=./operator-sdk
                    CURL_FOLLOW_REDIRECTS="-L"
                    curl ${CURL_FOLLOW_REDIRECTS} https://github.com/operator-framework/operator-sdk/releases/download/${RELEASE_VERSION}/operator-sdk-${RELEASE_VERSION}-x86_64-linux-gnu -o ${DOWNLOAD_LOCATION}
                    chmod +x ${DOWNLOAD_LOCATION}
                name: Download operator-sdk
            - run:
                command: |
                    export OPERATOR_TAG="${LATEST_TAG}"
                    export MONITOR_TAG="${LATEST_TAG}-ubi8"
                    python3 scripts/operator/create_operator_and_push.py "${OPERATOR_TAG}" "${MONITOR_TAG}" "${DOCKERHUB_USER}" "${DOCKERHUB_PASSWORD}"
                    echo "export OPERATOR_TAG=${OPERATOR_TAG}" >> $BASH_ENV
                name: Create Operator and push Operator image to DockerHub
            - snyk/scan:
                docker-image-name: snyk/kubernetes-operator:${OPERATOR_TAG}
                monitor-on-build: true
                severity-threshold: critical
                target-file: snyk-operator/build/Dockerfile
            - run:
                command: ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    push_operator_to_community_operators:
        description: |
            Packages a new Operator and pushes it to Snyk's fork of
            the OpenShift k8s-operatorhub/community-operators.
        executor: redhat-openshift/default
        steps:
            - checkout
            - add_ssh_keys:
                fingerprints:
                    - 5a:63:89:5d:a1:71:fb:a4:ed:c5:ec:4c:1e:0b:c7:3a
            - run:
                command: |
                    python -m pip install requests pyyaml PyGithub
                name: Install required packages
            - install_python_requests
            - run:
                command: |
                    set -xeo pipefail
                    OPERATOR_REPO_URL="https://raw.githubusercontent.com/k8s-operatorhub/community-operators/main/operators/snyk-operator/snyk-operator.package.yaml"

                    LAST_OPERATOR_VERSION=$(python ./scripts/operator/get_last_published_operator_version.py $OPERATOR_REPO_URL)
                    echo "export LAST_OPERATOR_VERSION=${LAST_OPERATOR_VERSION}" >> $BASH_ENV
                name: Get last released Operator version
            - run:
                command: |
                    LATEST_TAG_WITH_V=`git describe --abbrev=0 --tags ${CIRCLE_SHA1}`
                    LATEST_TAG=${LATEST_TAG_WITH_V:1}
                    NEW_OPERATOR_VERSION=${LATEST_TAG}
                    echo "export NEW_OPERATOR_VERSION=${NEW_OPERATOR_VERSION}" >> $BASH_ENV
                name: Get new Operator version
            - run:
                command: |
                    if [[ "${NEW_OPERATOR_VERSION}" == "${LAST_OPERATOR_VERSION}" ]]; then
                      echo "export NOTHING_TO_TEST=true" >> $BASH_ENV
                      exit 1
                    fi
                name: End tests early if no new Operator is to be released
            - run:
                command: |
                    SNYK_MONITOR_TAG="${NEW_OPERATOR_VERSION}-ubi8"
                    OPERATOR_PATH=$(python scripts/operator/package_operator_bundle.py "${NEW_OPERATOR_VERSION}" "${NEW_OPERATOR_VERSION}" "${SNYK_MONITOR_TAG}" "${LAST_OPERATOR_VERSION}")
                    echo "export OPERATOR_PATH=${OPERATOR_PATH}" >> $BASH_ENV
                name: Package Operator Bundle
            - run:
                command: |
                    CURRENT_DIRECTORY=$(pwd)
                    COMMUNITY_FOLDER_LOCATION="community-operators"
                    ./scripts/operator/push-operator-to-snyk-upstream.sh "${CURRENT_DIRECTORY}" "${COMMUNITY_FOLDER_LOCATION}"
                    echo "export COMMUNITY_FOLDER_LOCATION=${COMMUNITY_FOLDER_LOCATION}" >> $BASH_ENV
                name: Push new Operator to the fork of the k8s-operatorhub/community-operators repository
            - run:
                command: |
                    SLACK_PR_URL=$(python ./scripts/operator/raise_pr_to_community_operators_from_our_fork.py "${COMMUNITY_FOLDER_LOCATION}" "${NEW_OPERATOR_VERSION}")
                    echo "export SLACK_PR_URL=${SLACK_PR_URL}" >> $BASH_ENV
                name: Open a Pull Request to the k8s-operatorhub/community-operators repository
            - run:
                command: |
                    ./scripts/slack/notify_success_operator_push.py "${NEW_OPERATOR_VERSION}" "community/operators/${COMMUNITY_FOLDER_LOCATION}" "${SLACK_PR_URL}"
                name: Notify Slack on new branch in snyk/community-operators
            - run:
                command: |
                    if [[ "${NOTHING_TO_TEST}" != "true" ]]; then
                      ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                    fi
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    push_operator_to_embedded_community_operators:
        description: |
            Packages a new Operator and pushes it to Snyk's fork of
            the redhat-openshift-ecosystem/community-operators-prod.
        executor: redhat-openshift/default
        steps:
            - checkout
            - add_ssh_keys:
                fingerprints:
                    - 5a:63:89:5d:a1:71:fb:a4:ed:c5:ec:4c:1e:0b:c7:3a
            - run:
                command: |
                    python -m pip install requests pyyaml PyGithub
                name: Install required packages
            - install_python_requests
            - run:
                command: |
                    set -xeo pipefail
                    OPERATOR_REPO_URL="https://raw.githubusercontent.com/redhat-openshift-ecosystem/community-operators-prod/main/operators/snyk-operator/snyk-operator.package.yaml"

                    LAST_OPERATOR_VERSION=$(python ./scripts/operator/get_last_published_operator_version.py $OPERATOR_REPO_URL)
                    echo "export LAST_OPERATOR_VERSION=${LAST_OPERATOR_VERSION}" >> $BASH_ENV
                name: Get last released Operator version
            - run:
                command: |
                    LATEST_TAG_WITH_V=`git describe --abbrev=0 --tags ${CIRCLE_SHA1}`
                    LATEST_TAG=${LATEST_TAG_WITH_V:1}
                    NEW_OPERATOR_VERSION=${LATEST_TAG}
                    echo "export NEW_OPERATOR_VERSION=${NEW_OPERATOR_VERSION}" >> $BASH_ENV
                name: Get new Operator version
            - run:
                command: |
                    if [[ "${NEW_OPERATOR_VERSION}" == "${LAST_OPERATOR_VERSION}" ]]; then
                      echo "export NOTHING_TO_TEST=true" >> $BASH_ENV
                      exit 1
                    fi
                name: End tests early if no new Operator is to be released
            - run:
                command: |
                    SNYK_MONITOR_TAG="${NEW_OPERATOR_VERSION}-ubi8"
                    OPERATOR_PATH=$(python scripts/operator/package_operator_bundle.py "${NEW_OPERATOR_VERSION}" "${NEW_OPERATOR_VERSION}" "${SNYK_MONITOR_TAG}" "${LAST_OPERATOR_VERSION}")
                    echo "export OPERATOR_PATH=${OPERATOR_PATH}" >> $BASH_ENV
                name: Package Operator Bundle
            - run:
                command: |
                    CURRENT_DIRECTORY=$(pwd)
                    COMMUNITY_FOLDER_LOCATION="community-operators-prod"
                    ./scripts/operator/push-operator-to-snyk-upstream.sh "${CURRENT_DIRECTORY}" "${COMMUNITY_FOLDER_LOCATION}"
                    echo "export COMMUNITY_FOLDER_LOCATION=${COMMUNITY_FOLDER_LOCATION}" >> $BASH_ENV
                name: Push new Operator to the fork of the redhat-openshift-ecosystem/community-operators-prod repository
            - run:
                command: |
                    SLACK_PR_URL=$(python ./scripts/operator/raise_pr_to_embedded_community_operators_from_our_fork.py "${COMMUNITY_FOLDER_LOCATION}" "${NEW_OPERATOR_VERSION}")
                    echo "export SLACK_PR_URL=${SLACK_PR_URL}" >> $BASH_ENV
                name: Open a Pull Request to the redhat-openshift-ecosystem/community-operators-prod repository
            - run:
                command: |
                    ./scripts/slack/notify_success_operator_push.py "${NEW_OPERATOR_VERSION}" "${COMMUNITY_FOLDER_LOCATION}" "${SLACK_PR_URL}"
                name: Notify Slack on new branch in snyk/community-operators-prod
            - run:
                command: |
                    if [[ "${NOTHING_TO_TEST}" != "true" ]]; then
                      ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                    fi
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    sync_community_operators_with_snyk_fork:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/python:3.10
        steps:
            - checkout
            - install_python_requests
            - add_ssh_keys:
                fingerprints:
                    - 5a:63:89:5d:a1:71:fb:a4:ed:c5:ec:4c:1e:0b:c7:3a
            - run:
                command: |
                    CURRENT_DIRECTORY=$(pwd)
                    COMMUNITY_FOLDER_LOCATION="${CURRENT_DIRECTORY}/community-operators"

                    # Clone Community Operators repo from Snyk
                    git clone https://github.com/snyk/community-operators.git "${COMMUNITY_FOLDER_LOCATION}"
                    cd "${COMMUNITY_FOLDER_LOCATION}"

                    # Sync snyk/community-operators repo from k8s-operatorhub/community-operators repo
                    git remote add upstream https://github.com/k8s-operatorhub/community-operators.git
                    git fetch upstream
                    git merge upstream/main
                    git push origin main
                name: Sync k8s-operatorhub/community-operators with snyk/community-operators
            - run:
                command: ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    sync_embedded_community_operators_with_snyk_fork:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/python:3.10
        steps:
            - checkout
            - install_python_requests
            - add_ssh_keys:
                fingerprints:
                    - 5a:63:89:5d:a1:71:fb:a4:ed:c5:ec:4c:1e:0b:c7:3a
            - run:
                command: |
                    CURRENT_DIRECTORY=$(pwd)
                    COMMUNITY_FOLDER_LOCATION="${CURRENT_DIRECTORY}/community-operators-prod"

                    # Clone Community Operators repo from Snyk
                    git clone https://github.com/snyk/community-operators-prod.git "${COMMUNITY_FOLDER_LOCATION}"
                    cd "${COMMUNITY_FOLDER_LOCATION}"

                    # Sync snyk/community-operators repo from redhat-openshift-ecosystem/community-operators-prod repo
                    git remote add upstream https://github.com/redhat-openshift-ecosystem/community-operators-prod.git
                    git fetch upstream
                    git merge upstream/main
                    git push origin main
                name: Sync redhat-openshift-ecosystem/community-operators-prod with snyk/community-operators-prod
            - run:
                command: ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    system_tests:
        machine:
            docker_layer_caching: true
            image: ubuntu-2204:2022.04.1
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - run:
                command: |
                    export DEBIAN_FRONTEND=noninteractive
                    sudo apt-get update -qq
                    sudo apt-get install skopeo
                name: Install Skopeo
            - run:
                command: |
                    npm run build &&
                    npm run test:system
                name: System tests
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    tag_and_push:
        docker:
            - auth:
                password: $DOCKERHUB_PASSWORD
                username: $DOCKERHUB_USER
              image: cimg/node:16.13
        steps:
            - checkout
            - setup_remote_docker
            - install_python_requests
            - run:
                command: |
                    npm ci &&
                    docker login --username ${DOCKERHUB_USER} --password ${DOCKERHUB_PASSWORD} &&
                    unset CIRCLE_PULL_REQUEST &&
                    unset CI_PULL_REQUEST &&
                    unset CI_PULL_REQUESTS &&
                    unset CIRCLE_PULL_REQUESTS &&
                    npx semantic-release@17.2.2 &&
                    NEW_VERSION=`cat ./package.json | jq -r '.version'` &&
                    ./scripts/docker/approve-image.sh $NEW_VERSION
                name: Tag and push
            - run:
                command: ./scripts/slack/notify_failure.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
    unit_tests:
        machine:
            docker_layer_caching: true
            image: ubuntu-2004:202111-01
        steps:
            - checkout
            - setup_node16
            - install_python_requests
            - snyk/scan:
                monitor-on-build: false
                severity-threshold: high
            - run:
                command: |
                    npm run build &&
                    npm run test:unit
                name: Unit tests
            - run:
                command: |
                    ./scripts/slack/notify_failure_on_branch.py "${CIRCLE_BRANCH}" "${CIRCLE_JOB}" "${CIRCLE_BUILD_URL}" "${CIRCLE_PULL_REQUEST}" "${SLACK_WEBHOOK}"
                name: Notify Slack on failure
                when: on_fail
        working_directory: ~/kubernetes-monitor
main_branches_filter:
    filters:
        branches:
            ignore:
                - staging
                - master
master_branch_only_filter:
    filters:
        branches:
            only:
                - master
orbs:
    aws-cli: circleci/aws-cli@2.0.6
    azure-cli: circleci/azure-cli@1.2.0
    redhat-openshift: circleci/redhat-openshift@0.2.0
    snyk: snyk/snyk@1.1.2
staging_branch_only_filter:
    filters:
        branches:
            only:
                - staging
version: 2.1
workflows:
    # TODO(cmars): uncomment if this looks good to container team!
    #FEAT_MONITOR_GLOO_CRD:
    #    jobs:
    #        - build_image_snyk_main:
    #            context: <TODO: context for publishing to internal registry>
    #            filters:
    #              branches:
    #                only:
    #                  - feat/monitor-gloo-crd

    MERGE_TO_MASTER:
        jobs:
            - publish:
                context: nodejs-app-release-public
                filters:
                    branches:
                        only:
                            - master
            - deploy_to_prod:
                filters:
                    branches:
                        only:
                            - master
                requires:
                    - publish
    MERGE_TO_STAGING:
        jobs:
            - build_image:
                filters:
                    branches:
                        only:
                            - staging
            - build_and_upload_operator:
                filters:
                    branches:
                        only:
                            - staging
            - unit_tests:
                filters:
                    branches:
                        only:
                            - staging
            - system_tests:
                filters:
                    branches:
                        only:
                            - staging
            - integration_tests:
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - build_image
            - integration_tests_helm:
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - build_image
            - integration_tests_proxy:
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - build_image
            - eks_integration_tests:
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - build_image
            - aks_integration_tests:
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - build_image
            - integration_tests_operator_on_k8s:
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - build_image
                    - build_and_upload_operator
            - tag_and_push:
                context: nodejs-app-release-public
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - build_image
                    - build_and_upload_operator
                    - unit_tests
                    - system_tests
                    - integration_tests
                    - integration_tests_helm
                    - integration_tests_proxy
            - deploy_to_dev:
                context: nodejs-app-release-public
                filters:
                    branches:
                        only:
                            - staging
                requires:
                    - tag_and_push
    PR_TO_STAGING:
        jobs:
            - build_image:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
            - build_and_upload_operator:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
            - unit_tests:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
            - lint:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
            - code_formatter:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
            - system_tests:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
            - integration_tests:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
                requires:
                    - build_image
            - integration_tests_helm:
                filters:
                    branches:
                        ignore:
                            - staging
                            - master
                requires:
                    - build_image

